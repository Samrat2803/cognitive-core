"""
Infographic Data Schemas

6 standardized input schemas that agents use to populate infographic templates.
Each schema type serves a different purpose, but all can be rendered in any visual template.

Schema Type → Data Structure (constant)
Visual Template → Design Style (variable)
"""

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime


# ============================================================================
# SCHEMA TYPE 1: KEY METRICS DASHBOARD
# ============================================================================
# Use for: Overall scores, KPIs, sentiment results, summary statistics
# Templates: Works with all 6 (especially 1, 3, 4, 5)

class MetricItem(BaseModel):
    """Single metric/KPI"""
    value: str = Field(..., description="The metric value (e.g., '75%', '1.2M', '+15%')")
    label: str = Field(..., description="What this metric represents")
    description: Optional[str] = Field(None, description="Additional context")


class KeyMetricsDashboard(BaseModel):
    """Dashboard showing 2-6 key metrics"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="Secondary description")
    metrics: List[MetricItem] = Field(..., min_items=2, max_items=6, description="2-6 key metrics")
    insight: Optional[str] = Field(None, description="Key takeaway or insight")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "US Climate Policy Sentiment",
                "subtitle": "Real-time Analysis Across Media Sources",
                "metrics": [
                    {"value": "72%", "label": "Overall Support", "description": "Across all demographics"},
                    {"value": "1.2M", "label": "Articles Analyzed", "description": "Last 30 days"},
                    {"value": "+15%", "label": "Weekly Change", "description": "Trending upward"},
                    {"value": "28", "label": "Countries Tracked", "description": "Global coverage"}
                ],
                "insight": "Public sentiment shows strong support for renewable energy initiatives, with bipartisan approval increasing.",
                "footer": "Generated by Political Analysis AI • Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 2: COMPARISON VIEW
# ============================================================================
# Use for: Side-by-side comparisons, A vs B, before/after, source comparison
# Templates: Works with all 6 (especially 2, 3, 5)

class ComparisonSide(BaseModel):
    """One side of the comparison"""
    title: str = Field(..., description="Name of this side (e.g., 'Source A', 'Before', 'Left')")
    metrics: List[MetricItem] = Field(..., max_items=5, description="Up to 5 metrics for this side")
    summary: Optional[str] = Field(None, description="Brief summary")


class ComparisonView(BaseModel):
    """Side-by-side comparison of two entities"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being compared")
    left_side: ComparisonSide = Field(..., description="Left/first comparison")
    right_side: ComparisonSide = Field(..., description="Right/second comparison")
    conclusion: Optional[str] = Field(None, description="Comparison conclusion")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Media Bias: CNN vs Fox News",
                "subtitle": "Coverage of Climate Policy - Sept 2025",
                "left_side": {
                    "title": "CNN",
                    "metrics": [
                        {"value": "68%", "label": "Positive Coverage"},
                        {"value": "1,240", "label": "Articles Published"},
                        {"value": "Left-Center", "label": "Bias Rating"}
                    ],
                    "summary": "Predominantly supportive coverage"
                },
                "right_side": {
                    "title": "Fox News",
                    "metrics": [
                        {"value": "34%", "label": "Positive Coverage"},
                        {"value": "980", "label": "Articles Published"},
                        {"value": "Right-Center", "label": "Bias Rating"}
                    ],
                    "summary": "More critical perspective"
                },
                "conclusion": "34-point sentiment gap between outlets on this issue",
                "footer": "Media Bias Detector • Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 3: TIMELINE/PROGRESSION
# ============================================================================
# Use for: Time-series data, historical trends, event sequences, evolution
# Templates: Works with all 6 (especially 2, 5, 6)

class TimelinePoint(BaseModel):
    """Single point in time"""
    date: str = Field(..., description="Date or time label (e.g., 'Jan 2025', 'Week 1', 'Phase 1')")
    value: str = Field(..., description="Metric value at this time")
    label: str = Field(..., description="What changed or happened")
    description: Optional[str] = Field(None, description="Additional context")


class TimelineProgression(BaseModel):
    """Timeline showing progression over time"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being tracked")
    timeline_points: List[TimelinePoint] = Field(..., min_items=2, max_items=6, description="2-6 time points")
    trend: Optional[str] = Field(None, description="Overall trend description")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Climate Policy Sentiment Evolution",
                "subtitle": "Public Opinion Trends - 2024-2025",
                "timeline_points": [
                    {"date": "Jan 2024", "value": "52%", "label": "Initial Support", "description": "Baseline measurement"},
                    {"date": "Jun 2024", "value": "58%", "label": "Rising Interest", "description": "After major wildfires"},
                    {"date": "Dec 2024", "value": "65%", "label": "Growing Momentum", "description": "Policy debates intensify"},
                    {"date": "Oct 2025", "value": "72%", "label": "Strong Support", "description": "Current measurement"}
                ],
                "trend": "Steady 20-point increase over 21 months, accelerating in recent quarters",
                "footer": "Sentiment Analyzer • Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 4: RANKING/LEADERBOARD
# ============================================================================
# Use for: Top N lists, rankings, entity comparison, source credibility
# Templates: Works with all 6 (especially 2, 4, 5)

class RankedItem(BaseModel):
    """Single ranked item"""
    rank: int = Field(..., ge=1, description="Position in ranking (1 = highest)")
    name: str = Field(..., description="Name of entity being ranked")
    score: str = Field(..., description="Score or metric")
    change: Optional[str] = Field(None, description="Change from previous (e.g., '+2', '↑', 'New')")
    description: Optional[str] = Field(None, description="Brief description")


class RankingLeaderboard(BaseModel):
    """Ranked list of entities"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being ranked")
    ranked_items: List[RankedItem] = Field(..., min_items=2, max_items=10, description="2-10 ranked items")
    methodology: Optional[str] = Field(None, description="How ranking was determined")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Most Credible News Sources",
                "subtitle": "Fact-Checking Accuracy - Sept 2025",
                "ranked_items": [
                    {"rank": 1, "name": "Reuters", "score": "94.2%", "change": "—", "description": "Highest accuracy"},
                    {"rank": 2, "name": "AP News", "score": "93.8%", "change": "↑1", "description": "Improved this month"},
                    {"rank": 3, "name": "BBC News", "score": "91.5%", "change": "↓1", "description": "Slight decrease"},
                    {"rank": 4, "name": "The Guardian", "score": "89.3%", "change": "—", "description": "Consistent performance"},
                    {"rank": 5, "name": "NPR", "score": "88.7%", "change": "New", "description": "First appearance"}
                ],
                "methodology": "Based on fact-check verification and correction rate",
                "footer": "Fact Checker Agent • Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 5: SINGLE HERO STAT
# ============================================================================
# Use for: One major finding, headline number, key announcement
# Templates: Works with all 6 (especially 1, 4, 6)

class HeroStat(BaseModel):
    """Single prominent statistic with context"""
    title: str = Field(..., description="Main headline or question")
    hero_value: str = Field(..., description="The big number/stat (e.g., '87%', '$2.4B', '10x')")
    hero_label: str = Field(..., description="What the number represents")
    supporting_stats: Optional[List[MetricItem]] = Field(None, max_items=3, description="0-3 supporting metrics")
    context: str = Field(..., description="Explanation or context paragraph")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Election 2024 Prediction Accuracy",
                "hero_value": "94.7%",
                "hero_label": "Prediction Accuracy",
                "supporting_stats": [
                    {"value": "342/361", "label": "Races Predicted Correctly"},
                    {"value": "0.8%", "label": "Margin of Error"},
                    {"value": "15M", "label": "Data Points Analyzed"}
                ],
                "context": "Our predictive model achieved record accuracy in the 2024 election cycle, correctly forecasting 94.7% of all races across federal, state, and local levels.",
                "footer": "Predictive Analytics • Nov 2024"
            }
        }


# ============================================================================
# SCHEMA TYPE 6: GEOGRAPHIC/CATEGORY BREAKDOWN
# ============================================================================
# Use for: Regional analysis, demographic breakdown, category distribution
# Templates: Works with all 6 (especially 3, 5, 6)

class CategoryItem(BaseModel):
    """Single category or region"""
    name: str = Field(..., description="Category/region name")
    value: str = Field(..., description="Metric for this category")
    percentage: Optional[str] = Field(None, description="Percentage of total (e.g., '32%')")
    description: Optional[str] = Field(None, description="Brief description")


class CategoryBreakdown(BaseModel):
    """Breakdown by categories or geography"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being broken down")
    breakdown_type: str = Field(..., description="Type (e.g., 'Geographic', 'Demographic', 'Categorical')")
    categories: List[CategoryItem] = Field(..., min_items=2, max_items=8, description="2-8 categories")
    total_label: Optional[str] = Field(None, description="Label for total (e.g., 'Total Responses')")
    total_value: Optional[str] = Field(None, description="Total value across all categories")
    insight: Optional[str] = Field(None, description="Key takeaway")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Climate Policy Support by Region",
                "subtitle": "Geographic Sentiment Analysis - USA",
                "breakdown_type": "Geographic",
                "categories": [
                    {"name": "West Coast", "value": "82%", "percentage": "26%", "description": "Highest support"},
                    {"name": "Northeast", "value": "76%", "percentage": "22%", "description": "Strong support"},
                    {"name": "Midwest", "value": "58%", "percentage": "19%", "description": "Moderate support"},
                    {"name": "South", "value": "51%", "percentage": "18%", "description": "Mixed views"},
                    {"name": "Southwest", "value": "63%", "percentage": "15%", "description": "Growing support"}
                ],
                "total_label": "National Average",
                "total_value": "66%",
                "insight": "31-point spread between highest (West Coast) and lowest (South) regions",
                "footer": "Geo-Sentiment Analyzer • Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA REGISTRY
# ============================================================================

INFOGRAPHIC_SCHEMAS = {
    "key_metrics": KeyMetricsDashboard,
    "comparison": ComparisonView,
    "timeline": TimelineProgression,
    "ranking": RankingLeaderboard,
    "hero_stat": HeroStat,
    "category_breakdown": CategoryBreakdown
}


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_schema_by_type(schema_type: str) -> BaseModel:
    """
    Get schema class by type name
    
    Args:
        schema_type: One of 'key_metrics', 'comparison', 'timeline', 'ranking', 'hero_stat', 'category_breakdown'
    
    Returns:
        Pydantic model class
    """
    if schema_type not in INFOGRAPHIC_SCHEMAS:
        raise ValueError(f"Unknown schema type: {schema_type}. Valid types: {list(INFOGRAPHIC_SCHEMAS.keys())}")
    return INFOGRAPHIC_SCHEMAS[schema_type]


def validate_infographic_data(schema_type: str, data: Dict[str, Any]) -> BaseModel:
    """
    Validate data against schema
    
    Args:
        schema_type: Schema type name
        data: Data to validate
    
    Returns:
        Validated Pydantic model instance
    
    Raises:
        ValidationError if data doesn't match schema
    """
    schema_class = get_schema_by_type(schema_type)
    return schema_class(**data)


def get_example_data(schema_type: str) -> Dict[str, Any]:
    """
    Get example data for a schema type
    
    Args:
        schema_type: Schema type name
    
    Returns:
        Example data dict
    """
    schema_class = get_schema_by_type(schema_type)
    return schema_class.Config.json_schema_extra["example"]


# ============================================================================
# USAGE EXAMPLES
# ============================================================================

if __name__ == "__main__":
    print("🎨 Infographic Schema System")
    print("=" * 70)
    print()
    
    print("Available Schema Types:")
    for i, (key, schema) in enumerate(INFOGRAPHIC_SCHEMAS.items(), 1):
        print(f"  {i}. {key:<20} → {schema.__name__}")
    print()
    
    # Example 1: Key Metrics Dashboard
    print("📊 Example 1: Key Metrics Dashboard")
    print("-" * 70)
    example1 = KeyMetricsDashboard(
        title="US Climate Policy Sentiment",
        subtitle="Real-time Analysis",
        metrics=[
            MetricItem(value="72%", label="Overall Support"),
            MetricItem(value="1.2M", label="Articles Analyzed"),
            MetricItem(value="+15%", label="Weekly Change"),
            MetricItem(value="28", label="Countries Tracked")
        ],
        insight="Strong bipartisan support for renewable energy"
    )
    print(f"Title: {example1.title}")
    print(f"Metrics: {len(example1.metrics)} items")
    print(f"Valid: ✅")
    print()
    
    # Example 2: Comparison
    print("⚖️  Example 2: Comparison View")
    print("-" * 70)
    example2 = ComparisonView(
        title="Media Bias Comparison",
        subtitle="CNN vs Fox News",
        left_side=ComparisonSide(
            title="CNN",
            metrics=[MetricItem(value="68%", label="Positive Coverage")]
        ),
        right_side=ComparisonSide(
            title="Fox News",
            metrics=[MetricItem(value="34%", label="Positive Coverage")]
        ),
        conclusion="34-point sentiment gap"
    )
    print(f"Title: {example2.title}")
    print(f"Left: {example2.left_side.title} - {example2.left_side.metrics[0].value}")
    print(f"Right: {example2.right_side.title} - {example2.right_side.metrics[0].value}")
    print(f"Valid: ✅")
    print()
    
    # Example 3: Validation Error
    print("❌ Example 3: Validation Error Handling")
    print("-" * 70)
    try:
        invalid = KeyMetricsDashboard(
            title="Test",
            subtitle="Test",
            metrics=[
                MetricItem(value="72%", label="Only One Metric")
                # ERROR: Need at least 2 metrics!
            ]
        )
    except Exception as e:
        print(f"Caught validation error: {type(e).__name__}")
        print("✅ Schema validation working correctly")
    print()
    
    print("🎉 Schema system ready to use!")
    print()
    print("Next steps:")
    print("1. Agents use these schemas to structure their data")
    print("2. Pass schema + template choice to renderer")
    print("3. Get beautiful infographic in chosen style")

