"""
Infographic Data Schemas

6 standardized input schemas that agents use to populate infographic templates.
Each schema type serves a different purpose, but all can be rendered in any visual template.

Schema Type â†’ Data Structure (constant)
Visual Template â†’ Design Style (variable)
"""

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime


# ============================================================================
# SCHEMA TYPE 1: KEY METRICS DASHBOARD
# ============================================================================
# Use for: Overall scores, KPIs, sentiment results, summary statistics
# Templates: Works with all 6 (especially 1, 3, 4, 5)

class MetricItem(BaseModel):
    """Single metric/KPI"""
    value: str = Field(..., description="The metric value (e.g., '75%', '1.2M', '+15%')")
    label: str = Field(..., description="What this metric represents")
    description: Optional[str] = Field(None, description="Additional context")


class KeyMetricsDashboard(BaseModel):
    """Dashboard showing 2-6 key metrics"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="Secondary description")
    metrics: List[MetricItem] = Field(..., min_items=2, max_items=6, description="2-6 key metrics")
    insight: Optional[str] = Field(None, description="Key takeaway or insight")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "US Climate Policy Sentiment",
                "subtitle": "Real-time Analysis Across Media Sources",
                "metrics": [
                    {"value": "72%", "label": "Overall Support", "description": "Across all demographics"},
                    {"value": "1.2M", "label": "Articles Analyzed", "description": "Last 30 days"},
                    {"value": "+15%", "label": "Weekly Change", "description": "Trending upward"},
                    {"value": "28", "label": "Countries Tracked", "description": "Global coverage"}
                ],
                "insight": "Public sentiment shows strong support for renewable energy initiatives, with bipartisan approval increasing.",
                "footer": "Generated by Political Analysis AI â€¢ Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 2: COMPARISON VIEW
# ============================================================================
# Use for: Side-by-side comparisons, A vs B, before/after, source comparison
# Templates: Works with all 6 (especially 2, 3, 5)

class ComparisonSide(BaseModel):
    """One side of the comparison"""
    title: str = Field(..., description="Name of this side (e.g., 'Source A', 'Before', 'Left')")
    metrics: List[MetricItem] = Field(..., max_items=5, description="Up to 5 metrics for this side")
    summary: Optional[str] = Field(None, description="Brief summary")


class ComparisonView(BaseModel):
    """Side-by-side comparison of two entities"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being compared")
    left_side: ComparisonSide = Field(..., description="Left/first comparison")
    right_side: ComparisonSide = Field(..., description="Right/second comparison")
    conclusion: Optional[str] = Field(None, description="Comparison conclusion")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Media Bias: CNN vs Fox News",
                "subtitle": "Coverage of Climate Policy - Sept 2025",
                "left_side": {
                    "title": "CNN",
                    "metrics": [
                        {"value": "68%", "label": "Positive Coverage"},
                        {"value": "1,240", "label": "Articles Published"},
                        {"value": "Left-Center", "label": "Bias Rating"}
                    ],
                    "summary": "Predominantly supportive coverage"
                },
                "right_side": {
                    "title": "Fox News",
                    "metrics": [
                        {"value": "34%", "label": "Positive Coverage"},
                        {"value": "980", "label": "Articles Published"},
                        {"value": "Right-Center", "label": "Bias Rating"}
                    ],
                    "summary": "More critical perspective"
                },
                "conclusion": "34-point sentiment gap between outlets on this issue",
                "footer": "Media Bias Detector â€¢ Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 3: TIMELINE/PROGRESSION
# ============================================================================
# Use for: Time-series data, historical trends, event sequences, evolution
# Templates: Works with all 6 (especially 2, 5, 6)

class TimelinePoint(BaseModel):
    """Single point in time"""
    date: str = Field(..., description="Date or time label (e.g., 'Jan 2025', 'Week 1', 'Phase 1')")
    value: str = Field(..., description="Metric value at this time")
    label: str = Field(..., description="What changed or happened")
    description: Optional[str] = Field(None, description="Additional context")


class TimelineProgression(BaseModel):
    """Timeline showing progression over time"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being tracked")
    timeline_points: List[TimelinePoint] = Field(..., min_items=2, max_items=6, description="2-6 time points")
    trend: Optional[str] = Field(None, description="Overall trend description")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Climate Policy Sentiment Evolution",
                "subtitle": "Public Opinion Trends - 2024-2025",
                "timeline_points": [
                    {"date": "Jan 2024", "value": "52%", "label": "Initial Support", "description": "Baseline measurement"},
                    {"date": "Jun 2024", "value": "58%", "label": "Rising Interest", "description": "After major wildfires"},
                    {"date": "Dec 2024", "value": "65%", "label": "Growing Momentum", "description": "Policy debates intensify"},
                    {"date": "Oct 2025", "value": "72%", "label": "Strong Support", "description": "Current measurement"}
                ],
                "trend": "Steady 20-point increase over 21 months, accelerating in recent quarters",
                "footer": "Sentiment Analyzer â€¢ Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 4: RANKING/LEADERBOARD
# ============================================================================
# Use for: Top N lists, rankings, entity comparison, source credibility
# Templates: Works with all 6 (especially 2, 4, 5)

class RankedItem(BaseModel):
    """Single ranked item"""
    rank: int = Field(..., ge=1, description="Position in ranking (1 = highest)")
    name: str = Field(..., description="Name of entity being ranked")
    score: str = Field(..., description="Score or metric")
    change: Optional[str] = Field(None, description="Change from previous (e.g., '+2', 'â†‘', 'New')")
    description: Optional[str] = Field(None, description="Brief description")


class RankingLeaderboard(BaseModel):
    """Ranked list of entities"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being ranked")
    ranked_items: List[RankedItem] = Field(..., min_items=2, max_items=10, description="2-10 ranked items")
    methodology: Optional[str] = Field(None, description="How ranking was determined")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Most Credible News Sources",
                "subtitle": "Fact-Checking Accuracy - Sept 2025",
                "ranked_items": [
                    {"rank": 1, "name": "Reuters", "score": "94.2%", "change": "â€”", "description": "Highest accuracy"},
                    {"rank": 2, "name": "AP News", "score": "93.8%", "change": "â†‘1", "description": "Improved this month"},
                    {"rank": 3, "name": "BBC News", "score": "91.5%", "change": "â†“1", "description": "Slight decrease"},
                    {"rank": 4, "name": "The Guardian", "score": "89.3%", "change": "â€”", "description": "Consistent performance"},
                    {"rank": 5, "name": "NPR", "score": "88.7%", "change": "New", "description": "First appearance"}
                ],
                "methodology": "Based on fact-check verification and correction rate",
                "footer": "Fact Checker Agent â€¢ Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA TYPE 5: SINGLE HERO STAT
# ============================================================================
# Use for: One major finding, headline number, key announcement
# Templates: Works with all 6 (especially 1, 4, 6)

class HeroStat(BaseModel):
    """Single prominent statistic with context"""
    title: str = Field(..., description="Main headline or question")
    hero_value: str = Field(..., description="The big number/stat (e.g., '87%', '$2.4B', '10x')")
    hero_label: str = Field(..., description="What the number represents")
    supporting_stats: Optional[List[MetricItem]] = Field(None, max_items=3, description="0-3 supporting metrics")
    context: str = Field(..., description="Explanation or context paragraph")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Election 2024 Prediction Accuracy",
                "hero_value": "94.7%",
                "hero_label": "Prediction Accuracy",
                "supporting_stats": [
                    {"value": "342/361", "label": "Races Predicted Correctly"},
                    {"value": "0.8%", "label": "Margin of Error"},
                    {"value": "15M", "label": "Data Points Analyzed"}
                ],
                "context": "Our predictive model achieved record accuracy in the 2024 election cycle, correctly forecasting 94.7% of all races across federal, state, and local levels.",
                "footer": "Predictive Analytics â€¢ Nov 2024"
            }
        }


# ============================================================================
# SCHEMA TYPE 6: GEOGRAPHIC/CATEGORY BREAKDOWN
# ============================================================================
# Use for: Regional analysis, demographic breakdown, category distribution
# Templates: Works with all 6 (especially 3, 5, 6)

class CategoryItem(BaseModel):
    """Single category or region"""
    name: str = Field(..., description="Category/region name")
    value: str = Field(..., description="Metric for this category")
    percentage: Optional[str] = Field(None, description="Percentage of total (e.g., '32%')")
    description: Optional[str] = Field(None, description="Brief description")


class CategoryBreakdown(BaseModel):
    """Breakdown by categories or geography"""
    title: str = Field(..., description="Main headline")
    subtitle: str = Field(..., description="What's being broken down")
    breakdown_type: str = Field(..., description="Type (e.g., 'Geographic', 'Demographic', 'Categorical')")
    categories: List[CategoryItem] = Field(..., min_items=2, max_items=8, description="2-8 categories")
    total_label: Optional[str] = Field(None, description="Label for total (e.g., 'Total Responses')")
    total_value: Optional[str] = Field(None, description="Total value across all categories")
    insight: Optional[str] = Field(None, description="Key takeaway")
    footer: Optional[str] = Field("Generated by Political Analysis AI", description="Attribution text")
    
    class Config:
        json_schema_extra = {
            "example": {
                "title": "Climate Policy Support by Region",
                "subtitle": "Geographic Sentiment Analysis - USA",
                "breakdown_type": "Geographic",
                "categories": [
                    {"name": "West Coast", "value": "82%", "percentage": "26%", "description": "Highest support"},
                    {"name": "Northeast", "value": "76%", "percentage": "22%", "description": "Strong support"},
                    {"name": "Midwest", "value": "58%", "percentage": "19%", "description": "Moderate support"},
                    {"name": "South", "value": "51%", "percentage": "18%", "description": "Mixed views"},
                    {"name": "Southwest", "value": "63%", "percentage": "15%", "description": "Growing support"}
                ],
                "total_label": "National Average",
                "total_value": "66%",
                "insight": "31-point spread between highest (West Coast) and lowest (South) regions",
                "footer": "Geo-Sentiment Analyzer â€¢ Oct 2, 2025"
            }
        }


# ============================================================================
# SCHEMA REGISTRY
# ============================================================================

INFOGRAPHIC_SCHEMAS = {
    "key_metrics": KeyMetricsDashboard,
    "comparison": ComparisonView,
    "timeline": TimelineProgression,
    "ranking": RankingLeaderboard,
    "hero_stat": HeroStat,
    "category_breakdown": CategoryBreakdown
}


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_schema_by_type(schema_type: str) -> BaseModel:
    """
    Get schema class by type name
    
    Args:
        schema_type: One of 'key_metrics', 'comparison', 'timeline', 'ranking', 'hero_stat', 'category_breakdown'
    
    Returns:
        Pydantic model class
    """
    if schema_type not in INFOGRAPHIC_SCHEMAS:
        raise ValueError(f"Unknown schema type: {schema_type}. Valid types: {list(INFOGRAPHIC_SCHEMAS.keys())}")
    return INFOGRAPHIC_SCHEMAS[schema_type]


def validate_infographic_data(schema_type: str, data: Dict[str, Any]) -> BaseModel:
    """
    Validate data against schema
    
    Args:
        schema_type: Schema type name
        data: Data to validate
    
    Returns:
        Validated Pydantic model instance
    
    Raises:
        ValidationError if data doesn't match schema
    """
    schema_class = get_schema_by_type(schema_type)
    return schema_class(**data)


def get_example_data(schema_type: str) -> Dict[str, Any]:
    """
    Get example data for a schema type
    
    Args:
        schema_type: Schema type name
    
    Returns:
        Example data dict
    """
    schema_class = get_schema_by_type(schema_type)
    return schema_class.Config.json_schema_extra["example"]


# ============================================================================
# USAGE EXAMPLES
# ============================================================================

if __name__ == "__main__":
    print("ðŸŽ¨ Infographic Schema System")
    print("=" * 70)
    print()
    
    print("Available Schema Types:")
    for i, (key, schema) in enumerate(INFOGRAPHIC_SCHEMAS.items(), 1):
        print(f"  {i}. {key:<20} â†’ {schema.__name__}")
    print()
    
    # Example 1: Key Metrics Dashboard
    print("ðŸ“Š Example 1: Key Metrics Dashboard")
    print("-" * 70)
    example1 = KeyMetricsDashboard(
        title="US Climate Policy Sentiment",
        subtitle="Real-time Analysis",
        metrics=[
            MetricItem(value="72%", label="Overall Support"),
            MetricItem(value="1.2M", label="Articles Analyzed"),
            MetricItem(value="+15%", label="Weekly Change"),
            MetricItem(value="28", label="Countries Tracked")
        ],
        insight="Strong bipartisan support for renewable energy"
    )
    print(f"Title: {example1.title}")
    print(f"Metrics: {len(example1.metrics)} items")
    print(f"Valid: âœ…")
    print()
    
    # Example 2: Comparison
    print("âš–ï¸  Example 2: Comparison View")
    print("-" * 70)
    example2 = ComparisonView(
        title="Media Bias Comparison",
        subtitle="CNN vs Fox News",
        left_side=ComparisonSide(
            title="CNN",
            metrics=[MetricItem(value="68%", label="Positive Coverage")]
        ),
        right_side=ComparisonSide(
            title="Fox News",
            metrics=[MetricItem(value="34%", label="Positive Coverage")]
        ),
        conclusion="34-point sentiment gap"
    )
    print(f"Title: {example2.title}")
    print(f"Left: {example2.left_side.title} - {example2.left_side.metrics[0].value}")
    print(f"Right: {example2.right_side.title} - {example2.right_side.metrics[0].value}")
    print(f"Valid: âœ…")
    print()
    
    # Example 3: Validation Error
    print("âŒ Example 3: Validation Error Handling")
    print("-" * 70)
    try:
        invalid = KeyMetricsDashboard(
            title="Test",
            subtitle="Test",
            metrics=[
                MetricItem(value="72%", label="Only One Metric")
                # ERROR: Need at least 2 metrics!
            ]
        )
    except Exception as e:
        print(f"Caught validation error: {type(e).__name__}")
        print("âœ… Schema validation working correctly")
    print()
    
    print("ðŸŽ‰ Schema system ready to use!")
    print()
    print("Next steps:")
    print("1. Agents use these schemas to structure their data")
    print("2. Pass schema + template choice to renderer")
    print("3. Get beautiful infographic in chosen style")

